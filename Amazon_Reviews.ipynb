{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrerubeis/Sentiment-Analysis-Project-NLP/blob/main/Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pickle5\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnsaJPpR1Zwk",
        "outputId": "c1831f0c-2f47-4421-ae4b-21ee936d9b4f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 92 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 133 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 153 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 163 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 174 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 184 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 194 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 204 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 215 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 225 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 235 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 245 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 18.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.12\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 10.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "!git init\n",
        "\n",
        "if not os.path.isdir('./sa'):\n",
        "  !git clone https://github.com/andrerubeis/Sentiment-Analysis-Project-NLP.git\n",
        "  !mv 'Sentiment-Analysis-Project-NLP' './sa'\n"
      ],
      "metadata": {
        "id": "HrJ5NNFI8Lls",
        "outputId": "fb6b9667-3614-4dac-9275-d9c51b4f44ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized empty Git repository in /content/.git/\n",
            "Cloning into 'Sentiment-Analysis-Project-NLP'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 46 (delta 11), reused 37 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data_utils\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from torchtext.legacy import data\n",
        "import pickle5 as pickle\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sa import dataset\n",
        "from sa import params\n",
        "from sa import net\n",
        "#from sa import *\n",
        "#import sa.params\n",
        "#from sa import net\n",
        "#from sa import dataset\n",
        "\n",
        "#from dataset import AmazonDataset\n"
      ],
      "metadata": {
        "id": "MLddvTBA4uPe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "yLQaaBWxfTap"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = dataset.AmazonDataset() # fare prova con anche dataset non caricato\n",
        "#dataset.load_dataset()"
      ],
      "metadata": {
        "id": "oTr2Q-hTPbAp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problemi:\n",
        "1. Stiamo passando alla rete le frasi di testo (dataset.data) non tensori. In che formato vanno passati al modello? getitem restituisce una lista di elementi, dove ogni elemento è un tensore (ogni tensore corrisponde ad una parola della recenzione e ha lunghezza 3072) e la lunghezza della recensione è variabile\n",
        "2. Attention weight va ancora chiamato\n",
        "3. In dataset.embedded_words_dict ci sono parole (keys) con \"##\" davanti, credo non ci debbano essere? (vedi cella sotto)"
      ],
      "metadata": {
        "id": "tXJIPOiDJV1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ci sono parole che hanno \"##\" davanti, credo vadano eliminate\n",
        "print(dataset.embedded_words_dict['##o'])"
      ],
      "metadata": {
        "id": "C_AUhjFzYASW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.data[0])\n",
        "print(dataset.__getitem__(0))\n",
        "print(dataset.labels[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "QeOlyxxf3ghU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.labels, train_size=0.7, random_state=0, shuffle = True)\n",
        "\n",
        "train_data = { 'features': x_train, 'labels': y_train}\n",
        "test_data = {'features': x_test, 'labels': y_test}\n",
        "\n",
        "train_data = pd.DataFrame(train_data)\n",
        "test_data = pd.DataFrame(test_data)\n"
      ],
      "metadata": {
        "id": "pZoAXraSUWan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "batch_size = params.BATCH_SIZE\n",
        "hidden_dim = 128\n",
        "embedding_size = len(dataset.__getitem__(0)[0]) #lunghezza embedding (selezionato uno a caso perchè tanto tutti hanno la stessa dimensione)\n",
        "dropout_rate = params.DROPOUT_RATE\n",
        "lr = params.LR\n",
        "epochs = params.NUM_EPOCHS\n",
        "best_loss = float('inf')\n",
        "\n",
        "# BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "# x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, train_size=0.8, random_state=0)\n",
        "train_iterator, test_iterator = data.BucketIterator.splits((train_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    sort_key = lambda x: len(x.text), # Sort the batches by text length size\n",
        "    sort_within_batch = True,\n",
        "    device = device)\n",
        "\n",
        "# Build the model\n",
        "lstm_model = net.SentimentAnalysis(batch_size,\n",
        "                                   hidden_dim,\n",
        "                                   embedding_size,\n",
        "                                   dropout_rate) #modificato LSTM con SentimentAnalysis (nome rete)\n",
        "\n",
        "# optimization algorithm\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "bRyMu10RVfrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_train = True\n",
        "\n",
        "# train and validate\n",
        "if (to_train):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # training \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for idx, batch in enumerate(train_iterator):\n",
        "            optimizer.zero_grad()\n",
        "            # retrieve text and no. of words\n",
        "            #text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = lstm_model(batch)    #(text, text_lengths) # batch_size, hidden_dim, vocab_size, window, dropout_rate\n",
        "            loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels.squeeze())+ 0.5*nn.MSELoss(predictions, batch.labels.squeeze())\n",
        "\n",
        "            winners = predictions.argmax(dim=1)\n",
        "            corrects = (winners == batch.labels)\n",
        "            accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "            # perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += accuracy.item()\n",
        "            \n",
        "        train_loss, train_acc = epoch_loss / len(train_iterator), epoch_acc / len(train_iterator)\n",
        "\n",
        "        # save best model\n",
        "        if train_loss < best_loss:\n",
        "            best_valid_loss = train_loss\n",
        "            torch.save(lstm_model.state_dict(), 'saved_weights_BiLSTM.pt')\n",
        "\n",
        "        print(f'\\tEpoch; {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')  "
      ],
      "metadata": {
        "id": "piGVJCc1r3t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load weights and make predictions\n",
        "lstm_model.load_state_dict(torch.load(\"saved_weights_BiLSTM.pt\"))\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "lstm_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = lstm_model(batch).squeeze(1)\n",
        "\n",
        "        loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels)+ 0.5*nn.MSELoss(predictions, batch.labels)\n",
        "\n",
        "        winners = predictions.argmax(dim=1)\n",
        "        corrects = (winners == batch.labels)\n",
        "        accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += accuracy.item()\n",
        "\n",
        "test_loss, test_acc =  epoch_loss / len(test_iterator), epoch_acc / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "fHlRh2BGW1sq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}